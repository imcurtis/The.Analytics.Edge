wordsAdded = tm_map(wordsAdded, removeWords, c(stopwords("english")))
wordsAdded = tm_map(wordsAdded, stemDocument)
wordsAdded = DocumentTermMatrix(wordsAdded)
wordsRemoved = as.data.frame(as.matrix(wordsAdded))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
wordsRemoved
sum(wordsRemoved == 1)
wikiWords$Vandal = wiki$Vandal
wikiWords = cbind(wordsAdded, wordsRemoved)
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
trainWikiwWords = subset(wikiWords, split==TRUE)
testWikiWords = subset(wikiWords, split==FALSE)
table(testWikiWords)
testWikiWords = subset(wikiWords, split==FALSE)
trainWikiwWords = subset(wikiWords, split==TRUE)
table(testWikiWords)
table(testWikiWords$Vandal)
table(testWikiWords$vandal)
table(wikiWords$Vandal)
table(testwikiWords$Vandal)
table(testWikiWords$Vandal)
2061(2061+1815)
2061/(2061+1815)
trainWikiWords = subset(wikiWords, split==TRUE)
testWikiWords = subset(wikiWords, split==FALSE)
library(rpart)
library(rpart.plot)
wikiCART = rpart(Vandal ~ ., data=trainWikiWords, method="class")
prp(wikiCART)
wikiTrain = subset(wikiWords, split==TRUE)
wikiTest = subset(wikiWords, split==FALSE)
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
wikiTrain = subset(wikiWords, split==TRUE)
wikiTest = subset(wikiWords, split==FALSE)
table(testWikiWords$Vandal)
2061/(2061+1815)
library(rpart)
library(rpart.plot)
wikiCART = rpart(Vandal ~ ., data=wikiTrain, method="class")
prp(wikiCART)
wikiCART = rpart(Vandal ~ ., wikiTrain, method="class")
prp(wikiCART)
table(wikiTrains$Vandal)
table(wikiTrain$Vandal)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
wikiTrain = subset(wikiWords, split==TRUE)
wikiTest = subset(wikiWords, split==FALSE)
table(wikiTrain$Vandal)
library(rpart)
library(rpart.plot)
wikiCART = rpart(Vandal ~ ., wikiTrain, method="class")
table(wikiTrain$Vandal)
wiki$Vandal = as.factor(wiki$Vandal)
sum(wiki$Vandal == 1)
wiki = read.csv("wiki.csv", stringsAsFactors=FALSE)
wiki$Vandal = as.factor(wiki$Vandal)
sum(wiki$Vandal == 1)
#1.2
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded, removeWords, c(stopwords("english")))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
#1.3
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
#1.4
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
wordsAdded
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
#1.5
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
wikiTrain = subset(wikiWords, split==TRUE)
wikiTest = subset(wikiWords, split==FALSE)
table(wikiTrain$Vandal)
2061/(2061+1815)
#1.6
library(rpart)
library(rpart.plot)
wikiCART = rpart(Vandal ~ ., wikiTrain, method="class")
prp(wikiCART)
wikiLog = glm(Vandal ~ ., data=wikiTrain, family="binomial")
predictions = predict(wikiLog, newdata=wikiTrain, type="response")
table(wikiTest$Vandal, predictions > 0.5)
wikiLog = glm(Vandal ~ ., data=wikiTest, family="binomial")
predictions = predict(wikiLog, newdata=wikiTest, type="response")
table(wikiTest$Vandal, predictions > 0.5)
(612+140)/(612+140+405+6)
table(wikiTest$Vandal, predictions)
table(wikiTest$Vandal, predictions)
predictions = predict(wikiCART, newdata=wikiTest, type="class")
table(wikiTest$Vandal, predictions)
(618+12)/(612+12+533)
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
wikiWords2$HTTP
sum(wikiWords2$HTTP == 1)
wikiTrain2 = subset(wikiWords2, split==TRUE)
wikiTest2 = subset(wikiWords2, split==FALSE)
wikiCART2 = rpart(Vandal ~., wikiTrain2, method="class")
predictions = predict(wikiCART2, newdata=wikiTest2, type="class")
table(wikiTest2$Vandal, predictions)
(609+57)/(609+57+488)
(609+57)/(609+57+488+9)
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
wikiWords2$NumWordsAdded
mean(wikiWords2$NumWordsAdded)
prp(wikiCART2)
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
mean(wikiWords2$NumWordsAdded)
wikiTrain2 = subset(wikiWords2, split==TRUE)
wikiTest2 = subset(wikiWords2, split==FALSE)
wikiCART2 = rpart(Vandal ~., wikiTrain2, method="class")
prp(wikiCART2)
predictions = predict(wikiCART2, newdata=wikiTest2, type="class")
table(wikiTest2$Vandal, predictions)
(514+248)/(514+248+104+297)
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
wikiCART3 = rpart(Vandal ~., wikiTrain3, method="class")
prp(wikiCART3)
predictions = predict(wikiCART3, newdata=wikiTest3, type="class")
table(wikiTest3$Vandal, predictions)
wikiTrain3 = subset(wikiWords3, split==TRUE)
wikiTest3 = subset(wikiWords3, split==FALSE)
wikiCART3 = rpart(Vandal ~., wikiTrain3, method="class")
prp(wikiCART3)
predictions = predict(wikiCART3, newdata=wikiTest3, type="class")
table(wikiTest3$Vandal, predictions)
(595+241)/(595+241+304+23)
clinical_trial <- read.csv("clinical_trial.csv")
which.max(clinical_trial$abstract)
nchar(clinical_trial$abstract[476])
nchar(clinical_trial$abstract)
clinical_trial$abstract[476]
nchar(clinical_trial$abstract[476], type = "chars", allowNA = FALSE, keepNA = NA)
nchar(clinical_trial$abstract[476], type = "chars", allowNA = FALSE)
nchar(clinical_trial$abstract[476], type = "chars")
nchar(as.character(clinical_trial$abstract[476]))
max(nchar(trials$abstract))
max(nchar(clincal_trial$abstract))
max(nchar(clinical_trial$abstract))
clinical_trial <- read.csv("clinical_trial.csv", stringsAsFactors=FALSE)
which.max(clinical_trial$abstract)
nchar(as.character(clinical_trial$abstract[476]))
nchar(clinical_trial$abstract)
max(nchar(clinical_trial$abstract))
sum(nchar(clinical_trial$abstract) == 0)
min(nchar(clinical_trial$titel))
min(nchar(clinical_trial$title))
which.minmin(nchar(clinical_trial$title))
which.min(nchar(clinical_trial$title))
clinical_trial$title[1258]
corpusTitle <- Corpus(VectorSource(clinical_trial$title))
corpusAbstract <- Corpus(VectorSource(clinical_trial$abstract))
corpusTitle = tm_map(corpusTitle, tolower)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, c(stopwords("english")))
corpusAbstract = tm_map(corpusAbstract, removeWords, c(stopwords("english")))
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusAbstract = tm_map(corpusAbstract, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
(stopwords("english"))
dtmTitle = removeSparseTerms(dtmTitle, 0.995)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.995)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmTitle = removeSparseTerms(dtmTitle, 0.995)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.995)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
(stopwords("english"))
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
(stopwords("english"))
length(stopwords("english"))
summary(dtmTitle)
str(dtmTitle)
str(dtmAbstrac)
str(dtmAbstract)
colSums(dtmAbstract)
max(colSums(dtmAbstract))
which.max(colSums(dtmAbstract))
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
colnames(dtmTitle)
colnames(dtmAbstract)
dtm = cbind(dtmTitle, dtmAbstract)
dtm = cbind(dtmTitle, dtmAbstract, clinical_trial$trial)
colSums(dtm)
sum(colSums(dtm))
str(colSums(dtm))
str(dtm)
library(caTools)
set.seed(144)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
train = subset(tweetsSparse, split==TRUE)
test = subset(tweetsSparse, split==FALSE)
table(dtm)
table(dtm$trial)
dtm = cbind(dtmTitle, dtmAbstract)
dtm$trial = clinical_trial$trial
library(caTools)
set.seed(144)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
train = subset(tweetsSparse, split==TRUE)
test = subset(tweetsSparse, split==FALSE)
table(dtm$trial)
1043/(817+1043)
table(train$trial)
table(train$trial)
library(caTools)
set.seed(144)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
train = subset(tweetsSparse, split==TRUE)
test = subset(tweetsSparse, split==FALSE)
table(train$trial)
1043/(817+1043)
library(caTools)
set.seed(144)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
train = subset(dtm, split==TRUE)
test = subset(dtm, split==FALSE)
table(train$trial)
trialCART = rpart(trial ~ ., data=train, method="class")
prp(tweetCART)
trialCART = rpart(trial ~ ., data=train, method="class")
prp(trialCART)
trialLog = glm(trial ~ ., data=train, family="binomial")
table(trialLog)
predictions = predict(trialLog, newdata=test, type="response")
table(trialLog)
table(trialLog, predictions)
yes
y
trialLog = glm(trial ~ ., data=train, family="binomial")
predictions = predict(trialLog, newdata=test, type="response")
table(trialLog, predictions)
predictCART = predict(trialCART, newdata=trial, type="class")
predictCART = predict(trialCART, newdata=train, type="class")
table(predictCART)
717/(717+579)
predTrain=predict(trialCART)[,2]
predTrain
table(predTrain)
predTrain=predict(trialCART)[,2]
summary(predTrain)
(train$trial, predTrain >= 0.5)
table(train$trial, predTrain >= 0.5)
(464+610)/(464+610+115+107)
610/(610+115)
464/(464+107)
predictCART = predict(trialCART, newdata=train, type="class")
predTest = predict(trialCART, newdata=test, type="response")
trialCART = glm(trial ~ ., data=train, family="binomial")
predictCART = predict(trialCART, newdata=train, type="class")
predTrain=predict(trialCART)[,2]
summary(predTrain)
table(train$trial, predTrain >= 0.5)
(464+610)/(464+610+115+107)
464/(464+107)
610/(610+115)
#4.1
predictCART = predict(trialCART, newdata=train, type="class")
trialCART = glm(trial ~ ., data=train, family="binomial")
predTest = predict(trialCART, newdata=test, type="response")
table(trialCART$trial, predTest >= 0.5)
predTest = predict(trialCART, newdata=test, type="response")
table(trialCART$trial, predTest >= 0.5)
table(train$trial, predTrain >= 0.5)
table(test$trial, predTest >= 0.5)
(250+167)/(250+167+68+79)
predTest = predict(trialCART, newdata=test, type="response")[,2]
table(test$trial, predTest >= 0.5)
require(ROCR)
ROCRpred <- prediction(predTest, test$trial)
auc <- as.numeric(performance(ROCRpred, "auc")@y.values)
auc
require(ROCR)
pred = prediction(predTest, test$trial)
auc <- as.numeric(performance(ROCRpred, "auc")@y.values)
auc
emails <- read.csv("emails.csv", stringsAsFactors=FALSE)
str(emails)
(emails$spam == 1)
sume(emails$spam == 1)
sum(emails$spam == 1)
(emails$spam[1])
(emails$text[1])
which.max(emails$text)
max(nchar(emails$text))
which.min(emails$text)
which.min((ncharemails$text))
which.min((nchar)emails$text)
which.min((nchar)emails$text)
which.min(nchar(emails$text))
corpus = Corpus(VectorSource(emails$text))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("english"))
corpus = tm_map(corpus, stemDocument)
dtm = DocumentTermMatrix(corpus)
length(stopwords("english"))
tm_map(corpus, removeWords, sw)
corpus = tm_map(corpus, stemDocument)
dtm = DocumentTermMatrix(corpus)
length(stopwords("english"))
str(dtm)
corpus = Corpus(VectorSource(emails$text))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
dtm = DocumentTermMatrix(corpus)
length(stopwords("english"))
str(dtm)
dtm
summary(dtm)
corpus = Corpus(VectorSource(emails$text))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
dtm = DocumentTermMatrix(corpus)
summary(dtm)
dtm
spdtm = removeSparseTerms(dtm, 0.95)
spdtm
emailsSparse = as.data.frame(as.matrix(spdtm))
make.names(emailsSparse, unique = FALSE, allow_ = TRUE)
colSums(emailsSparse)
which.max(colSums(emails$Sparse))
sort(colSums(emails$Sparse))
colSums(sort(emails$Sparse))
colSums(which.max(emails$Sparse))
max(colSums(emails$Sparse))
emailsSparse = as.data.frame(as.matrix(spdtm))
colnames(emailsSparse) = make.names(colnames(emailsSparse))
which.max(colSums(emailsSparse))
emailsSparse$spam = emails$Negative
summary(emailsSparse)
colSums(emailsSparse)
sort(colSums(emailsSparse))
sort(colSums(emailsSparse, spam == 0))
sort(colSums(subset(emailsSparse, spam == 0)))
emailsSparse$spam = emails$spam
sort(colSums(emailsSparse))
sort(colSums(subset(emailsSparse, spam == 0)))
sort(colSums(subset(emailsSparse, spam == 1)))
split = sample.split(emailsSparse$spam, SplitRatio = 0.7)
train = subset(emailsSparse, split==TRUE)
test = subset(emailsSparse, split==FALSE)
spamLog = glm(spam ~ ., data=train, family="binomial")
spamCART = rpart(spam ~ ., data=train, method="class")
summary(spamLog)
predictions = predict(spamLog, newdata=train, type="response")
predictions
predictions < 0.00001
sum(predictions < 0.00001)
sum(predictions > 0.99999)
sum(0.99999 > predictions > 0.00001)
sum(0.99999 >= predictions >= 0.00001)
sum(predictions)
predictions = predict(spamLog, newdata=train, type="response")
sum(predictions < 0.00001)
sum(predictions > 0.99999)
sum(predictions)
sum(predictions < 0.99999 && > 0.00001)
table(predTrainLog >= 0.00001 & predTrainLog <= 0.99999)
table(predictions >= 0.00001 & predictions <= 0.99999)
table(predictions > 0.99999)
table(predictions < 0.00001)
summary(spamLog)
predTrainCART = predict(spamCART)[,2]
predTrainCART
prp(predTrainCART)
library(rpart)
library(rpart.plot)
prp(predTrainCART)
library(rpart)
library(rpart.plot)
spamCART = rpart(spam ~ ., data=train, method="class")
predTrainCART = predict(spamCART)[,2]
prp(predTrainCART)
prp(spamCART)
table(emailsSparse$spam, predictions > 0.5)
predictions = predict(spamLog, newdata=train, type="response")
table(train$spam, predictions > 0.5)
(3052+954)/(3052+954+4)
pred = prediction(predictions, train$spam)
require(ROCR)
pred = prediction(predictions, train$spam)
auc <- as.numeric(performance(ROCRpred, "auc")@y.values)
auc
predTrainCART = predict(spamCART, type="class")[,2]
spamCART = rpart(spam ~ ., data=train, method="class")
predTrainCART = predict(spamCART)[,2]
table(train$spam, predTrainCART)
predTrainCART = predict(spamCART, type="class")
table(train$spam, predTrainCART)
(2877+896)/nrow(train)
require(ROCR)
predictionTrainCART = prediction(predTrainCART, train$spam)
auc <- as.numeric(performance(ROCRpred, "auc")@y.values)
auc
auc <- as.numeric(performance(predictionTrainCART, "auc")@y.values)
require(ROCR)
predictionTrainCART = prediction(predTrainCART, train$spam)
auc <- as.numeric(performance(predictionTrainCART, "auc")@y.values)
auc
require(ROCR)
predictionTrainCART = prediction(predTrainCART, train$spam)
predictionTrainCART = predict(predTrainCART)
predictionTrainCART = predict(predTrainCART, train$spam)
predictionTrainCART = predict(predTrainCART, newdata=train)
predictionTrainCART = predictiction(predTrainCART, newdata=train)
predictionTrainCART = prediction(predTrainCART, newdata=train)
predictionTrainCART = prediction(predTrainCART, train$spam)
library(ROCR)
predictionTrainCART = prediction(predTrainCART, train$spam)
ROCRpred = prediction(predTrainCART, train$spam)
predTrainCART = predict(spamCART, type="class")
table(train$spam, predTrainCART)
require(ROCR)
pred = prediction(predictions, train$spam)
auc <- as.numeric(performance(pred, "auc")@y.values)
auc
require(ROCR)
ROCRpred = prediction(predTrainCART, train$spam)
auc <- as.numeric(performance(predictionTrainCART, "auc")@y.values)
auc <- as.numeric(performance(ROCRpred, "auc")@y.values)
ROCRpred = prediction(predTrainCART, train$spam)
auc <- as.numeric(performance(ROCRpred, "auc")@y.values)
auc
predictionTrainCART = prediction(predTrainCART, train$spam)
as.numeric(performance(predictionTrainCART, "auc")@y.values)
auc
setwd("~/The Analytics Edge/Week 6")
movies = read.table("movieLens.txt.rft", header=FALSE, sep="|",quote="\"")
movies = read.table("movieLens.txt", header=FALSE, sep="|",quote="\"")
str(movies)
colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate", "IMDB", "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western")
str(movies)
colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate", "IMDB", "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western")
str(movies)
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL
colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate", "IMDB", "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western")
movies = unique(movies)
movies = read.table("movieLens.txt", header=FALSE, sep="|",quote="\"")
str(movies)
colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate", "IMDB", "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western")
movies$V11
sum(movies$V11)
summary(movies)
table(movies$Comedy)
table(movies$Western)
table(movies$Romance, movies$Drama)
distances = dist(movies[2:20], method = "euclidean")
dailykos <- read.csv("dailykos.csv")
distances = dist(dailykos, method = "euclidean")
