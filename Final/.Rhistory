1638 - 686
murders = read.csv("murders.csv")
str(murders)
statesMap = map_data("state")
str(statesMap)
ggplot(statesMap, aes(x = long, y = lat, group = group)) + geom_polygon(fill = "white", color = "black")
murders$region = tolower(murders$State)
murderMap = merge(statesMap, murders, by="region")
str(murderMap)
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = Murders)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend")
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = Population)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend")
murderMap$MurderRate = murderMap$Murders / murderMap$Population * 100000
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = MurderRate)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend")
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = MurderRate)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend", limits = c(0,10))
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = GunOwnership)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend")
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = Population)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend")
murderMap$GunOwnershipRate = murderMap$GunOwnership / murderMap$Population * 100000
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = GunOwnershipRate)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend")
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = MurderRate)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend", limits = c(0,10))
murderMap$GunOwnershipRate = murderMap$GunOwnership / murderMap$Population * 100000
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = GunOwnershipRate)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend", limits = c(0,10))
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = GunOwnership)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend", limits = c(0,10))
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = GunOwnership)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend", limits = c(0,10))
statesMap = map_data("state")
str(statesMap)
table(group)
table(statesMap$group)
ggplot(statesMap, aes(x = long, y = lat, group = group)) + geom_polygon(fill = "white", color = "black")
ggplot(statesMap, aes(x = long, y = lat, group = group)) + geom_polygon(fill = "white", color = "red")
ggplot(statesMap, aes(x = long, y = lat, group = group)) + geom_polygon(fill = "white", color = "black")
Train = subset(polling, Year == 2004 | Year == 2008)
polling <- read.csv("PollingImputed.csv")
Train = subset(polling, Year == 2004 | Year == 2008)
Test = subset(polling, Year == 2012)
mod2 = glm(Republican~SurveyUSA+DiffCount, data=Train, family="binomial")
TestPrediction = predict(mod2, newdata=Test, type="response")
TestPredictionBinary = as.numeric(TestPrediction > 0.5)
predictionDataFrame = data.frame(TestPrediction, TestPredictionBinary, Test$State)
predictionDataFrame
table(Test$State, TestPredictionBinary)
sum(TestPredictionBinary == 1)
sum(TestPrediction == 1)
mean(predictionDataFrame$TestPrediction)
predictionDataFrame$region = tolower(predictionDataFrame$Test.State)
predictionMap = merge(statesMap, predictionDataFrame, by = "region")
predictionMap = predictionMap[order(predictionMap$order),]
str(predictionMap)
str(statesMap)
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary)) + geom_polygon(color = "black")
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary))+ geom_polygon(color = "black") + scale_fill_gradient(low = "blue", high = "red", guide = "legend", breaks= c(0,1), labels = c("Democrat", "Republican"), name = "Prediction 2012")
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary)) + geom_polygon(color = "black")
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary))+ geom_polygon(color = "black") + scale_fill_gradient(low = "blue", high = "red", guide = "legend", breaks= c(0,1), labels = c("Democrat", "Republican"), name = "Prediction 2012")
table(TestPredictionBinary$Florida)
TestPrediction
str(TestPrediction)
predictionDataFrame = data.frame(TestPrediction, TestPredictionBinary, Test$State)
mean(predictionDataFrame$TestPrediction)
polling <- read.csv("PollingImputed.csv")
Train = subset(polling, Year == 2004 | Year == 2008)
Test = subset(polling, Year == 2012)
mod2 = glm(Republican~SurveyUSA+DiffCount, data=Train, family="binomial")
TestPrediction = predict(mod2, newdata=Test, type="response")
TestPredictionBinary = as.numeric(TestPrediction > 0.5)
predictionDataFrame = data.frame(TestPrediction, TestPredictionBinary, Test$State)
mean(predictionDataFrame$TestPrediction)
table(TestPredictionBinary)
mean(predictionDataFrame$TestPrediction)
(TestPrediction$Florida)
str(TestPrediction)
summary(TestPrediction)
predictionDataFrame
?geom.polygon
?geom_polygon
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary, linetype = 3)) + geom_polygon(color = "black")
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary, linetype = dotted)) + geom_polygon(color = "black")
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary, linetype = dashed)) + geom_polygon(color = "black")
?par
?geom_polygon
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary, size = 3)) + geom_polygon(color = "black")
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary, lty = 3)) + geom_polygon(color = "black")
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary, alpha = 0.3)) + geom_polygon(color = "black")
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary, alpha = 0.3)) + geom_polygon(color = "black")
edges <- read.csv("edges.csv")
users <- read.csv("users.csv")
str(users)
(str)edges
str(edges)
unique(edges)
unique(edges$V!)
unique(edges$V1)
146/40
unique(ed ges$V1)
unique(edges$V1)
(users$school)
table(users$school, users$locale)
table(users$gender, users$school)
g = graph.data.frame(edges, FALSE, users)
?graph.data.frame
?graph.data.frame
?graph.data.frame
g = graph.data.frame(edges, FALSE, users)
plot(g, vertex.size=5, vertex.label=NA)
This can be carried out with the following commands:
install.packages("igraph")
library(igraph)
g = graph.data.frame(edges, FALSE, users)
z
plot(g, vertex.size=5, vertex.label=NA)
degree(g)
degree(g)>10
degree(g)
degree(g)>=10
V(g)$size = degree(g)/2+2
plot(g, vertex.label=NA)
sort(g$size)
sort((g)$size)
V(g)$size
sort(V(g)$size)
V(g)$color = "black"
V(g)$color[V(g)$gender == "A"] = "red"
V(g)$color[V(g)$gender == "B"] = "gray"
V(g)$color = "black"
V(g)$color[V(g)$gender == "A"] = "red"
plot(g, vertex.label=NA)
V(g)$color = "black"
V(g)$color[V(g)$gender == "A"] = "red"
V(g)$color[V(g)$gender == "B"] = "gray"
V(g)$color = "black"
plot(g, vertex.label=NA)
str(users)
table(users$schoolc)
table(users$school)
V(g)$color = "black"
V(g)$color[V(g)$school == "A"] = "red"
V(g)$color[V(g)$school == "AB"] = "gray"
plot(g, vertex.label=NA)
table(users$locale)
V(g)$color = "black"
V(g)$color[V(g)$school == "A"] = "red"
V(g)$color[V(g)$school == "AB"] = "gray"
plot(g, vertex.label=NA)
V(g)$color = "black"
V(g)$color[V(g)$school == "A"] = "red"
V(g)$color[V(g)$school == "AB"] = "yellow"
plot(g, vertex.label=NA)
V(g)$color = "black"
V(g)$color[V(g)$school == "A"] = "red"
V(g)$color[V(g)$school == "AB"] = "gray"
plot(g, vertex.label=NA)
?igraph.plotting
setwd("~/The Analytics Edge/Week 7")
tweets <- read.csv("tweets.csv", stringsAsFactors=FALSE)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("english"))
frequencies = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(frequencies))
install.packages("tm")
library(tm)
install.packages("SnowballC")
library(SnowballC)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("english"))
frequencies = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(frequencies))
tweetfreq = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(frequencies))
allTweets = as.data.frame(as.matrix(tweetfreq))
str(allTweets)
frequencies = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(frequencies))
str(frequencies)
str(allTweets)
ncol(allTweets)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, (stopwords("english"))
frequencies = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(frequencies))
ncol(allTweets)
?wordcloud
install.packages("wordcloud")
?wordcloud
colSums(allTweets)
rowSums(allTweets)
?wordcloud
??wordcloud
wordcloud(allTweets)
wordcloud(allTweets, scale=c(4,.5))
install.packages(c("wordcloud","tm"),repos="http://cran.r-project.org")
library(wordcloud)
library(tm)
wordcloud(allTweets, scale=c(4,.5))
install.packages(c("wordcloud","tm"),repos="http://cran.r-project.org")
library(wordcloud)
library(tm)
wordcloud(allTweets, scale=c(4,.5))
allTweets = as.data.frame(as.matrix(frequencies))
wordcloud(allTweets, scale=c(4,.5))
wordcloud(allTweets$word, allTweets$freq, scale=c(4,.5))
wordcloud(allTweets, scale=c(4,.5), max.words=100)
frequencies = DocumentTermMatrix(corpus)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, (stopwords("english"))
frequencies = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(frequencies))
frequencies = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(frequencies))
wordcloud(allTweets, scale=c(4,.5), max.words=100)
install.packages(c("wordcloud","tm"),repos="http://cran.r-project.org")
library(wordcloud)
library(tm)
wordcloud(allTweets, scale=c(4,.5), max.words=100)
library(wordcloud)
library(tm)
wordcloud(allTweets, scale=c(4,.5), max.words=100)
myTDM <- TermDocumentMatrix(corpus)
wordcloud(allTweets, scale=c(4,.5), max.words=100)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(("apple", stopwords("english"))
frequencies = DocumentTermMatrix(corpus)
myTDM <- TermDocumentMatrix(corpus)
allTweets = as.data.frame(as.matrix(frequencies))
corpus = tm_map(corpus, removeWords, c("apple, stopwords("english")))
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
frequencies = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(frequencies))
wordcloud(allTweets)
wordcloud((colnames(allTweets))
wordcloud((colnames(allTweets), colSums(allTweets), scale=c(4, 0.5)
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(4, 0.5)
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(4, 0.5))
wordcloud(colnames(allTweets), colSums(allTweets))
wordcloud(colnames(allTweets), colSums(allTweets))
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("english")))
frequencies = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(frequencies))
wordcloud(colnames(allTweets), colSums(allTweets))
wordcloud(colnames(allTweets), colSums(allTweets))
??wordcloud
??wordcloud
wordcloud(colnames(allTweets$Avg == -1), colSums(allTweets$Avg == -1))
wordcloud(colnames(allTweets$Avg), colSums(allTweets))
allTweets = as.data.frame(as.matrix(frequencies))
wordcloud(colnames(allTweets$Avg), colSums(allTweets))
wordcloud(colnames(allTweets), colSums(allTweets))
??wordcloud
wordcloud(colnames(allTweets), colSums(allTweets), random.order=FALSE)
wordcloud(colnames(allTweets), colSums(allTweets), random.order=TRUE)
wordcloud(colnames(allTweets), colSums(allTweets), random.order=FALSE)
wordcloud(colnames(allTweets), colSums(allTweets), random.color=TRUE)
display.brewer.all()
wordcloud(colnames(allTweets), colSums(allTweets), colors=brewer.pal(9, "Blues")[c(5, 6, 7, 8, 9)], random.color=TRUE)
wordcloud(colnames(allTweets), colSums(allTweets), colors=brewer.pal(9, "Blues")[c(-5, -6, -7, -8, -9)], random.color=TRUE)
wordcloud(colnames(allTweets), colSums(allTweets), colors=brewer.pal(9, "Blues")[c(1, 2, 3, 4)], random.color=TRUE)
wordcloud(colnames(allTweets), colSums(allTweets), colors=brewer.pal(9, "Blues")[c(-1, -2, -3, -4)], random.color=TRUE)
setwd("~/The Analytics Edge/Final")
Movies <- read.csv("Movies.csv")
str(Movies)
MoviesTrain <- subset(Movies$Year < 2010)
MoviesTrain <- subset(Movies$Year < 2010, split=TRUE)
MoviesTrain <- subset(Movies, Year < 2010)
MoviesTest <- subset(Movies, Year >= 2010)
str(MoviesTrain)
str(MoviesTest)
lmMoviesTrain= lm(MoviesTrain[ , 3:ncol(MoviesTrain)])
lmMoviesTrain= lm(MoviesTrain ~ Rated + Runtime + Action + Adventure + Crime + Drama + Thriller + Fantasy + Horror + Sci.Fi + Comedy + Family + Mystery + Romance + Animation + Music + History + Documentary + Wins + Nominations + Production.Budget + Worldwide, data=MoviesTrain)
lmMoviesTrain = lm(Worldwide ~ Rated + Runtime + Action + Adventure + Crime + Drama + Thriller + Fantasy + Horror + Sci.Fi + Comedy + Family + Mystery + Romance + Animation + Music + History + Documentary + Wins + Nominations + Production.Budget, data=MoviesTrain)
summary(lmMoviesTrain)
cor(Worldwide, Production.Budget)
cor(MoviesTrain$Worldwide, MoviesTrain$Production.Budget)
lmMoviesTrain2 = lm(Worldwide ~ Runtime, Crime, Horror, Animation, History, Nominations, Production.Budget, data=MoviesTrain)
lmMoviesTrain2 = lm(Worldwide ~ Runtime + Crime + Horror + Animation + History + Nominations + Production.Budget, data=MoviesTrain)
summary(lmMoviesTrain2)
SSE = sum(MoviesTrain$Worldwide^2)
SSE
SSE = sum((MoviesTrain$Worldwide)^2)
SSE
predTest = predict(lmScore, newdata=MoviesTest)
predTest = predict(lmMoviesTrain2, newdata=MoviesTest)
summary(predTest)
sum((predTest-MoviesTest$Worldwide)^2, na.rm=TRUE)
baseline = mean(MoviesTrain$Worldwide)
SST = sum((baseline - MoviesTest$Worldwide)^2)
SST
lmMoviesTest = lm(Worldwide ~ Runtime + Crime + Horror + Animation + History + Nominations + Production.Budget, data=MoviesTrain)
summary(lmMoviesTest)
R2 = 1 - SSE/SST
R2
R2 = 1 - (SSE/SST)
R2
25.0006/60.43398
1-(25.0006/60.43398)
sum(Movies$Performance = Average)
sum(Movies$Performance = "Average")
Movies
Movies$Performance
str(Movies$Performance)
str(Movies)
Movies$Performance = factor(ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .75), "Excellent", ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .25), "Average", "Poor")))
Movies$Performance
sum(Movies$Performance == "Average")
sum(Movies$Performance == "Excellent")
Movies$Worldwide = NULL
set.seed(15071)
split = sample.split(Movies$Performance, SplitRatio = 0.7)
trainPerf = subset(tweetsSparse, split==TRUE)
testPerf = subset(tweetsSparse, split==FALSE)
testPerf = subset(Movies, split==FALSE)
split = sample.split(Movies$Performance, SplitRatio = 0.7)
set.seed(15071)
split = sample.split(Movies$Performance, SplitRatio = 0.7)
library(caTools)
set.seed(15071)
split = sample.split(Movies$Performance, SplitRatio = 0.7)
trainPerf = subset(Movies, split==TRUE)
testPerf = subset(Movies, split==FALSE)
lmMoviesTrain3 = lm(Performance ~ Rated + Runtime + Action + Adventure + Crime + Drama + Thriller + Fantasy + Horror + Sci.Fi + Comedy + Family + Mystery + Romance + Animation + Music + History + Documentary + Wins + Nominations + Production.Budget + Worldwide, data=MoviesTrain)
MoviesTrain
lmMoviesTrain3 = lm(Performance ~ Rated + Runtime + Action + Adventure + Crime + Drama + Thriller + Fantasy + Horror + Sci.Fi + Comedy + Family + Mystery + Romance + Animation + Music + History + Documentary + Wins + Nominations + Production.Budget + Worldwide, data=MoviesTrain)
str(MoviesTrain)
Movies$Performance = factor(ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .75), "Excellent", ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .25), "Average", "Poor")))
library(caTools)
set.seed(15071)
split = sample.split(Movies$Performance, SplitRatio = 0.7)
trainPerf = subset(Movies, split==TRUE)
testPerf = subset(Movies, split==FALSE)
Movies$Performance = factor(ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .75), "Excellent", ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .25), "Average", "Poor")))
str(MoviesTrain)
require(rpart)
library(rpart.plot)
CARTMoviesTrain3 = lm(Performance ~ Rated + Runtime + Action + Adventure + Crime + Drama + Thriller + Fantasy + Horror + Sci.Fi + Comedy + Family + Mystery + Romance + Animation + Music + History + Documentary + Wins + Nominations + Production.Budget + Worldwide, data=MoviesTrain, method="class")
Movies$Performance = factor(ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .75), "Excellent", ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .25), "Average", "Poor")))
Movies$Worldwide = NULL
Movies$Performance = factor(ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .75), "Excellent", ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .25), "Average", "Poor")))
set.seed(15071)
split = sample.split(Movies$Performance, SplitRatio = 0.7)
trainPerf = subset(Movies, split==TRUE)
testPerf = subset(Movies, split==FALSE)
require(rpart)
library(rpart.plot)
CARTMoviesTrain3 = lm(Performance ~ Rated + Runtime + Action + Adventure + Crime + Drama + Thriller + Fantasy + Horror + Sci.Fi + Comedy + Family + Mystery + Romance + Animation + Music + History + Documentary + Wins + Nominations + Production.Budget + Worldwide, data=MoviesTrain, method="class")
CARTMoviesTrain3 = lm(Performance ~ Rated + Runtime + Action + Adventure + Crime + Drama + Thriller + Fantasy + Horror + Sci.Fi + Comedy + Family + Mystery + Romance + Animation + Music + History + Documentary + Wins + Nominations + Production.Budget, data=MoviesTrain, method="class")
set.seed(15071)
split = sample.split(Movies$Performance, SplitRatio = 0.7)
CARTMoviesTrain3 = lm(Performance ~ Rated + Runtime + Action + Adventure + Crime + Drama + Thriller + Fantasy + Horror + Sci.Fi + Comedy + Family + Mystery + Romance + Animation + Music + History + Documentary + Wins + Nominations + Production.Budget, data=Movies, method="class")
CARTMoviesTrain3
CARTMoviesTrain3 = rpart(Performance ~ Rated + Runtime + Action + Adventure + Crime + Drama + Thriller + Fantasy + Horror + Sci.Fi + Comedy + Family + Mystery + Romance + Animation + Music + History + Documentary + Wins + Nominations + Production.Budget, data=Movies, method="class")
CARTMoviesTrain3
plot(CARTMoviesTrain3)
predCART = predict(CARTMoviesTrain3, newdata = MoviesTrain, type="class")
table(MoviesTrain$Performance, predCART)
predCART = predict(CARTMoviesTrain3, newdata = MoviesTest, type="class")
table(MoviesTrain$Performance, predCART)
prp(CARTMoviesTrain3)
predCART = predict(CARTMoviesTrain3, newdata = MoviesTrain, type="class")
table(MoviesTrain$Performance, predCART)
predCART = predict(CARTMoviesTrain3, newdata = Movies, type= "class")
table(MoviesTrain$Performance, predCART)
fedFunds <- read.csv("federalFundsRate.csv", stringsAsFactors=FALSE)
mean(fedFunds$RaisedFedFunds == "1")
which.max(fedFunds$Chairman)
str(fedFunds)
table(fedFunds$Chairman)
as.factor(fedFunds$Chairman)
as.factor(fedFunds$DemocraticPres)
as.factor(fedFunds$RaisedFedFunds)
set.seed(201)
library(caTools)
spl = sample.split(fedFunds$RaisedFedFunds, 0.7)
training = subset(fedFunds, spl=TRUE)
testing = subset(fedFunds, spl=FALSE)
lmTraining = lm(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data=training)
summary(lmTraining)
glmTraining = glm(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data=training)
summary(lmTraining)
glmTraining = glm(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data=training, family="binomial")
summary(lmTraining)
summary(glmTraining)
predict(glmTraining, newdata, type="response")
predict(am.glm, glmTraining, type="response")
newdata = data.frame(PreviousRate=-0.000394, Streak=0.149668, Unemployment=-0.059242, HomeownershipRate=-0.092466, DemocraticPres=0.397373, MonthsUntilElection=-0.004790)
predict(glmTraining, newdata, type="response")
newdata = data.frame(PreviousRate=.017, Streak=-3, Unemployment=-0.051, HomeownershipRate=0.653, DemocraticPres=0, MonthsUntilElection=18)
predict(glmTraining, newdata, type="response")
newdata = data.frame(PreviousRate=.017, Streak=-3, Unemployment=0.051, HomeownershipRate=0.653, DemocraticPres=0, MonthsUntilElection=18)
predict(glmTraining, newdata, type="response")
newdata = data.frame(PreviousRate=1.7, Streak=-3, Unemployment=5.1, HomeownershipRate=65.3, DemocraticPres=0, MonthsUntilElection=18)
predict(glmTraining, newdata, type="response")
newdata = data.frame(PreviousRate=1.7, Streak=-3, Unemployment=5.1, HomeownershipRate=65.3, DemocraticPres=1, MonthsUntilElection=18)
predict(glmTraining, newdata, type="response")
predictions = predict(glmTraining, newdata=testing, type="response")
predictions
table(predictions>0.5, testing$RaisedFedFunds)
table(training$RaisedFedFunds)
table(sign(training$Rasmussen))
sign(20)
table(sign(training$Rasmussen))
table(testing$RaisedFedFunds, predictions >= 0.5)
library(ROCR)
ROCRpred = prediction(predictions, testing$RaisedFedFunds)
as.numeric(performance(ROCRpred, "auc")@y.values)
library(ROCR)
PredictROC = predict(glmTraining, newdata=test)
pred = prediction(PredictROC, testing$RaisedFedFunds)
perf = performance(pred, "tpr", "fpr")
plot(perf)
library(ROCR)
PredictROC = predict(glmTraining, newdata=testing)
pred = prediction(PredictROC, testing$RaisedFedFunds)
perf = performance(pred, "tpr", "fpr")
plot(perf)
plot(perf, main="ROC curve", colorize=T)
auc(perform)
auc(perf)
as.numeric(performance(perf, "auc")@y.values)
perf
plot(perf, main="ROC curve", colorize=True)
plot(perf, main="ROC curve", colorize=TRUE)
set.seed(201)
library(caret)
library(e1071)
library(rpart)
library(rpart.plot)
fitTing = training( method = "cv", number = 10 )
cartGrid = expand.grid( .cp = (1:50)*0.001)
train(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = train, method = "rpart", trControl = fitTing, tuneGrid = cartGrid )
fitTing = trainControl( method = "cv", number = 10 )
cartGrid = expand.grid( .cp = (1:50)*0.001)
train(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = train, method = "rpart", trControl = fitTing, tuneGrid = cartGrid )
train(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = training, method = "rpart", trControl = fitTing, tuneGrid = cartGrid )
tree = rpart(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = training, method = "class", control = rpart.control(cp = 0.016))
prp(tree)
predictions1 = predict(tree, newdata=testing, type="class")
table(predictions1>0.5, testing$RaisedFedFunds)
table(predictions1 > 0.5, testing$RaisedFedFunds)
table(predictions1, testing$RaisedFedFunds)
Households <- read.csv("Households.csv")
str(Households)
table(Households$MorningPct, Households$NumVisits)
mean(Households$MorningPct)
mean(Households$AfternoonPct)
Household$NumVisits
Households$NumVisits
table(Households$NumVisits, Households$MorningPct)
mean(Households$MorningPct)
mean(Households$AfternoonPct)
28.73369*2500
.2873369*2500
.5144897*2500
sum(Households$MorningPct == 0)
sum(Households$AfternoonPct == 0)
150plus <- subset(Households$AvgSalesValue >= 150)
150plus <- subset(Households$AvgSalesValue >= $150)
150plus <- subset(Households, AvgSalesValue >= $150)
150plus <- subset(Households, AvgSalesValue >= 150)
Households$AvgSalesValue
mean(Households$NumVisits > 300)
Households$AvgDiscount
Disc25Plus <- subset(Households, AvgDiscount > 25)
which.min(Disc25Plus$AvgSalesValue)
nrow(Disc25Plus)
table(Disc25Plus)
View(Disc25Plus)
min(Disc25Plus$AvgSalesValue)
HouseholdsNorm = predict(preproc, Households)
library(caret)
preproc = preProcess(Households)
HouseholdsNorm = predict(preproc, Households)
max(Households$NumVisits)
min(Households$AfternoonPct)
HouseholdsNorm = predict(preproc, Households)
max(HouseholdsNorm$NumVisits)
min(HouseholdsNorm$AfternoonPct)
set.seed(200)
distances <- dist(HouseholdsNorm, method = "euclidean")
ClusterShoppers <- hclust(distances, method = "ward.D")
plot(ClusterShoppers, labels = FALSE)
set.seed(200)
KMC = kmeans(HouseholdsNorm, centers = 10)
table(KMC$cluster)
?centroid
plot(KMC$cluster)
mean(KMC$cluster)
center(KMC$cluster)
centers(KMC$cluster)
KMC$cluster
set.seed(5000)
KMC = kmeans(HouseholdsNorm, centers = 5)
table(KMC$cluster)
